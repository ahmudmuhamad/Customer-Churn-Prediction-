{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Preprocessing\n",
                "\n",
                "This notebook covers the data preprocessing steps for the Customer Churn Prediction project. We will load the split data, handle missing values, encode categorical variables, and scale numerical features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import joblib\n",
                "import os\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "# Set pandas display options\n",
                "pd.set_option('display.max_columns', None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_path = '../data/raw/splits'\n",
                "\n",
                "train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
                "val_df = pd.read_csv(os.path.join(data_path, 'validation.csv'))\n",
                "test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
                "\n",
                "print(f\"Train shape: {train_df.shape}\")\n",
                "print(f\"Val shape: {val_df.shape}\")\n",
                "print(f\"Test shape: {test_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "categorical_cols = [\n",
                "    'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',\n",
                "    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
                "    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
                "    'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
                "]\n",
                "\n",
                "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
                "\n",
                "target = 'Churn'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Cleaning\n",
                "\n",
                "Convert `TotalCharges` to numeric, coercing errors to NaN."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df['TotalCharges'] = pd.to_numeric(train_df['TotalCharges'], errors='coerce')\n",
                "val_df['TotalCharges'] = pd.to_numeric(val_df['TotalCharges'], errors='coerce')\n",
                "test_df['TotalCharges'] = pd.to_numeric(test_df['TotalCharges'], errors='coerce')\n",
                "\n",
                "print(\"Train Nulls after conversion:\")\n",
                "print(train_df[numerical_cols].isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Preprocessing Pipeline\n",
                "\n",
                "We will use `ColumnTransformer` to apply different transformations to numerical and categorical columns.\n",
                "We include `SimpleImputer` to handle missing values (like those introduced in `TotalCharges`).\n",
                "\n",
                "- **Numerical Columns**: `SimpleImputer(median)` -> `StandardScaler`\n",
                "- **Categorical Columns**: `SimpleImputer(most_frequent)` -> `OneHotEncoder`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define transformers\n",
                "numerical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
                "])\n",
                "\n",
                "# Combine transformers using ColumnTransformer\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numerical_transformer, numerical_cols),\n",
                "        ('cat', categorical_transformer, categorical_cols)\n",
                "    ],\n",
                "    remainder='drop'\n",
                ")\n",
                "\n",
                "# Fit the preprocessor on the training data ONLY\n",
                "preprocessor.fit(train_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Transform Data\n",
                "\n",
                "Transform both training and validation sets. **Test set is left untouched.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transform train and validation data\n",
                "X_train_transformed = preprocessor.transform(train_df)\n",
                "X_val_transformed = preprocessor.transform(val_df)\n",
                "\n",
                "# Get feature names\n",
                "onehot_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
                "all_features = numerical_cols + list(onehot_features)\n",
                "\n",
                "# Convert back to DataFrame\n",
                "X_train_processed = pd.DataFrame(X_train_transformed, columns=all_features)\n",
                "X_val_processed = pd.DataFrame(X_val_transformed, columns=all_features)\n",
                "\n",
                "print(f\"Processed Train shape: {X_train_processed.shape}\")\n",
                "print(f\"Processed Val shape: {X_val_processed.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Target Encoding\n",
                "\n",
                "Encode the target variable `Churn`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "le = LabelEncoder()\n",
                "\n",
                "# Fit on train and transform both train and val\n",
                "y_train_encoded = le.fit_transform(train_df[target])\n",
                "y_val_encoded = le.transform(val_df[target])\n",
                "\n",
                "# Add target back to processed dataframes (optional)\n",
                "X_train_processed[target] = y_train_encoded\n",
                "X_val_processed[target] = y_val_encoded\n",
                "\n",
                "print(\"Target Class Mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
                "print(f\"Processed Train with Target shape: {X_train_processed.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Processed Data\n",
                "\n",
                "Save the processed datasets to `data/processed` and the preprocessor to `models`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "processed_path = '../data/processed'\n",
                "models_path = '../models'\n",
                "os.makedirs(processed_path, exist_ok=True)\n",
                "os.makedirs(models_path, exist_ok=True)\n",
                "\n",
                "# Save processed data\n",
                "X_train_processed.to_csv(os.path.join(processed_path, 'train_processed.csv'), index=False)\n",
                "X_val_processed.to_csv(os.path.join(processed_path, 'val_processed.csv'), index=False)\n",
                "\n",
                "# Save the preprocessor and label encoder\n",
                "joblib.dump(preprocessor, os.path.join(models_path, 'preprocessor.joblib'))\n",
                "joblib.dump(le, os.path.join(models_path, 'label_encoder.joblib'))\n",
                "\n",
                "print(\"Data and models saved successfully.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}